{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.041x - Unit 2: Conditioning and Independence\n",
    "### Notes by Leo Robinovitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Core Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Conditional Probability:**\n",
    "  * Probabilities from a revised model that takes into account information about the outcome of a probabilistic experiment  \n",
    "  * Fundamental definition: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$ (valid when $P(B) \\geq 0$)\n",
    "  * Same rules apply:\n",
    "    * $P(A|B) \\geq 0$\n",
    "    * $P(\\Omega|B) = \\frac{P(\\Omega \\cap B)}{P(B)} = \\frac{P(B)}{P(B)} = 1$\n",
    "    * $P(B|B) = 1$\n",
    "    * Additivity: If $A \\cap C = \\emptyset$, then $P(A \\cup C | B) = P(A|B) + P(C|B)$?\n",
    "      * LHS: $\\frac{P((A \\cup C) \\cap B)}{P(B)} = \\frac{P((A \\cap B) \\cup (C \\cap B)))}{P(B)} = \\frac{P(A \\cap B) + P(C \\cap B)}{P(B)}$ because events are disjoint, and we can see that this matches RHS\n",
    "    * Therefore all axioms for probability ALSO true for conditional probabilities!\n",
    "    \n",
    "    \n",
    "  \n",
    "*  **Three Key Tools** for conditional probability:\n",
    "  1. Multiplication rule\n",
    "  2. Total probability theorem\n",
    "  3. Bayes' Rule (foundation of inference)  \n",
    "  \n",
    "  \n",
    "* **Multiplication Rule:**\n",
    "  * From definition of conditional probability: $P(A \\cap B) = P(B)P(A|B) = P(A)P(B|A)$\n",
    "  * Therefore, $P(A \\cap B \\cap C) = P((A \\cap B) \\cap C) = P(A \\cap B)*P(C|A \\cap B) = P(A)*P(B|A)*P(C|A \\cap B) \\to$ the probability of a given outcome on a leaf diagram is the multiplication of the probabilities of each path to get there\n",
    "  * Precisely, $P(A_1 \\cap A_2 ... \\cap A_n) = P(A_1)* \\prod\\limits_{i=2}^{n}P(A_i|A_1 ... \\cap A_{i-1})$  \n",
    "  \n",
    "  \n",
    "* **Total Probability Theorem:**\n",
    "  * Partition $\\Omega$ into $A_1, A_2, A_3...A_i$ and know $P(A_i)$ and $P(B|A_i)$ for every i\n",
    "  * $P(B) = \\sum\\limits_{i}P(A_i)P(B|A_i) \\to$ a weighted average where $P(A_i)$ are the weightings (note that sum of all $A_i\n",
    "  $'s add to 1)\n",
    "  * If i $\\to$ infinity, use countable additivity and replace summation with integral\n",
    "  \n",
    "  \n",
    "* **Bayes' Rule:**\n",
    "  * Again, partition $\\Omega$ into $A_1, A_2, A_3...A_i$ and know $P(A_i)$ and $P(B|A_i)$ for every i\n",
    "    * Here, $P(A_i) \\to$ \"Initial Beliefs\"\n",
    "  * Revised beliefs: $P(A_i|B) = \\frac{P(A_i \\cap B)}{P(B)}$\n",
    "    * Numerator: Multiplication Rule: $P(A_i \\cap B) = P(A_i)P(B|A_i)$\n",
    "    * Denominator: Total Probability Theorem: $P(B) = \\sum\\limits_{j}P(A_j)P(B|A_j)$\n",
    "  * Final Bayes Theorem: $P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{\\sum\\limits_{j}P(A_j)P(B|A_j)}$\n",
    "    * Foundation for inference: given model about world $P(B|A_i)$, draw conclusions about causes $P(A_i|B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Lecture 2 Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1 True or False:**\n",
    "\n",
    "*A. If Ω is finite and we have a discrete uniform probability law, and if B≠∅, then the conditional probability law on B, given that B occurred, is also discrete uniform.*\n",
    "* True, because the outcomes inside B maintain the same relative proportions as in the original probability law.\n",
    "\n",
    "*B. If Ω is finite and we have a discrete uniform probability law, and if B≠∅, then the conditional probability law on Ω, given that B occurred, is also discrete uniform.*\n",
    "* False. Outcomes in Ω that are outside B have zero conditional probability, so it cannot be the case that all outcomes in Ω have the same conditional probability.\n",
    "\n",
    "\n",
    "\n",
    "**#2 Let the sample space be the unit square, Ω=[0,1]^2, and let the probability of a set be the area of the set. Let A be the set of points (x,y)∈[0,1]2 for which y≤x. Let B be the set of points for which x≤1/2. Find P(A∣B).**\n",
    "\n",
    "* $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "  * $P(A \\cap B) = 1/2 * 1/2 * 1/2  = 1/8$\n",
    "  * $P(B) = 1/2$\n",
    "  * Therefore, $P(A|B) = 1/4$  \n",
    "  \n",
    "  \n",
    "**#3 True or False:**\n",
    "\n",
    "*A.* $P(A\\cap B \\cap C^c)=P(A \\cap B)P(C^c∣A \\cap B)$\n",
    "  * True\n",
    "  \n",
    "*B.* $P(A \\cap B \\cap C^c)=P(A)P(C^c∣A)P(B∣A \\cap C^c)$\n",
    "  * True\n",
    "  \n",
    "*C.* $P(A \\cap B \\cap C^c)=P(A)P(C^c \\cap A∣A)P(B∣A \\cap C^c)$\n",
    "  * True, because $P(C^c \\cap A∣A) = P(C^c|A)$\n",
    "  \n",
    "*D.* $P(A \\cap B∣C)=P(A∣C)P(B∣A \\cap C)$\n",
    "  * True --> application of multiplication rule $P(A \\cap B) = P(A)P(B|A)$ with extra prior event C\n",
    "  \n",
    "  \n",
    "**#4 We have an infinite collection of biased coins, indexed by the positive integers. Coin i has probability 2−i of being selected. A flip of coin i results in Heads with probability 3−i. We select a coin and flip it. What is the probability that the result is Heads? The geometric sum formula may be useful here:**  \n",
    "<img src=\"l2_ex4.png\", style=\"height:px;width:=250px;\">\n",
    "  \n",
    "* $P(Heads) = \\sum\\limits_{i=1}^{\\infty}P(A_i)P(Heads|A_i) = \\sum\\limits_{i=1}^{\\infty}2^{-i}3^{-i} = \\sum\\limits_{i=1}^{\\infty}\\frac{1}{6}^{i} = \\frac{1/6}{1-1/6} = \\frac{1}{5}$  \n",
    "\n",
    "\n",
    "**#5 A test for a certain rare disease is assumed to be correct 95% of the time: if a person has the disease, the test result is positive with probability 0.95, and if the person does not have the disease, the test result is negative with probability 0.95. A person drawn at random from a certain population has probability 0.001 of having the disease.**\n",
    "\n",
    "*A. Find the probability a random person tests positive:*\n",
    "  * $P(+|Has) = 0.95$\n",
    "  * $P(-|!Has) = 0.95$\n",
    "  * $P(Has) = 0.001$\n",
    "  * Infer $P(-|Has) = 0.05$, $P(+|!Has) = 0.05$, $P(!Has) = 0.999$\n",
    "  * Total Probability Theorem: $P(+) = P(Has)P(+|Has) + P(!Has)P(+|!Has) = 0.001*0.95 + 0.999*0.05 = 0.0509$\n",
    "  \n",
    "*B. Given that the person just tested positive, what is the probability he actually has the disease?*\n",
    "  * $P(Has|+) = \\frac{P(+|Has)P(Has)}{P(+)} = \\frac{0.95*0.001}{0.0509} = 0.01866 = 1.87\\%$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Lecture 3 Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Solved Problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Problem Set 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1 **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
