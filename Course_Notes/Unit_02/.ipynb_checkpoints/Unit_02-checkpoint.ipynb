{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.041x - Unit 2: Conditioning and Independence\n",
    "### Notes by Leo Robinovitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Core Concepts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Conditional Probability:**\n",
    "  * Probabilities from a revised model that takes into account information about the outcome of a probabilistic experiment  \n",
    "  * Fundamental definition: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$ (valid when $P(B) \\geq 0$)\n",
    "  * Same rules apply:\n",
    "    * $P(A|B) \\geq 0$\n",
    "    * $P(\\Omega|B) = \\frac{P(\\Omega \\cap B)}{P(B)} = \\frac{P(B)}{P(B)} = 1$\n",
    "    * $P(B|B) = 1$\n",
    "    * Additivity: If $A \\cap C = \\emptyset$, then $P(A \\cup C | B) = P(A|B) + P(C|B)$?\n",
    "      * LHS: $\\frac{P((A \\cup C) \\cap B)}{P(B)} = \\frac{P((A \\cap B) \\cup (C \\cap B)))}{P(B)} = \\frac{P(A \\cap B) + P(C \\cap B)}{P(B)}$ because events are disjoint, and we can see that this matches RHS\n",
    "    * Therefore all axioms for probability ALSO true for conditional probabilities!\n",
    "    \n",
    "    \n",
    "  \n",
    "*  **Three Key Tools** for conditional probability:\n",
    "  1. Multiplication Rule\n",
    "  2. Total Probability Theorem\n",
    "  3. Bayes' Rule (foundation of inference)  \n",
    "  \n",
    "  \n",
    "* **Multiplication Rule:**\n",
    "  * From definition of conditional probability: $P(A \\cap B) = P(B)P(A|B) = P(A)P(B|A)$\n",
    "  * Therefore, $P(A \\cap B \\cap C) = P((A \\cap B) \\cap C) = P(A \\cap B)*P(C|A \\cap B) = P(A)*P(B|A)*P(C|A \\cap B) \\to$ the probability of a given outcome on a leaf diagram is the multiplication of the probabilities of each path to get there\n",
    "  * Precisely, $P(A_1 \\cap A_2 ... \\cap A_n) = P(A_1)* \\prod\\limits_{i=2}^{n}P(A_i|A_1 ... \\cap A_{i-1})$  \n",
    "  \n",
    "  \n",
    "* **Total Probability Theorem:**\n",
    "  * Partition $\\Omega$ into $A_1, A_2, A_3...A_i$ and know $P(A_i)$ and $P(B|A_i)$ for every i\n",
    "  * $P(B) = \\sum\\limits_{i}P(A_i)P(B|A_i) \\to$ a weighted average where $P(A_i)$ are the weightings (note that sum of all $A_i\n",
    "  $'s add to 1)\n",
    "  * If i $\\to$ infinity, use countable additivity and replace summation with integral\n",
    "  \n",
    "  \n",
    "* **Bayes' Rule:**\n",
    "  * Again, partition $\\Omega$ into $A_1, A_2, A_3...A_i$ and know $P(A_i)$ and $P(B|A_i)$ for every i\n",
    "    * Here, $P(A_i) \\to$ \"Initial Beliefs\"\n",
    "  * Revised beliefs: $P(A_i|B) = \\frac{P(A_i \\cap B)}{P(B)}$\n",
    "    * Numerator: Multiplication Rule: $P(A_i \\cap B) = P(A_i)P(B|A_i)$\n",
    "    * Denominator: Total Probability Theorem: $P(B) = \\sum\\limits_{j}P(A_j)P(B|A_j)$\n",
    "  * Final Bayes Theorem: $P(A_i|B) = \\frac{P(A_i)P(B|A_i)}{\\sum\\limits_{j}P(A_j)P(B|A_j)}$\n",
    "    * Foundation for inference: given model about world $P(B|A_i)$, draw conclusions about causes $P(A_i|B)$  \n",
    "    \n",
    "    \n",
    "* **Independence:**\n",
    "  * Intuition: two events are independent if the occurrence of one event does not change our beliefs about the occurrence of the other\n",
    "    * Completely different from disjoint events! In fact, if $P(A)>0$ and $P(B)>0$, disjoint $A$ and $B$ cannot be independent\n",
    "  * If $P(B|A) = P(B)$, B is independent of A and vice versa\n",
    "  * As such, conditional probability/multiplication rule becomes $P(A \\cap B) = P(A)P(B|A) = P(A)P(B)$\n",
    "  * This is the true (symmetric) definition of independence: $\\mathbf{P(B \\cap A) = P(B)P(A)}$\n",
    "    * Note that this applies even when P(A) = 0 (normally conditional independence relies on that this not being true)\n",
    "  * If $A$ and $B$ are independent, than $A$ and $B^c$ are also independent (if two events are independent, their complements are also independent)\n",
    "  \n",
    "  \n",
    "* **Conditional Independence:**\n",
    "  * Given C, are A and B independent?\n",
    "  * Mathematically, $P(A \\cap B|C) = P(A|C)P(B|C)$?\n",
    "  * Cannot say that if A and B are independent, they remain independent under any C\n",
    "    * Example: Two coins, A and B. $P(A) = P(B) = 0.5$. Once chosen, $P(Heads|A) = 0.9$, and $P(Heads|B) = 0.1$. If independent, $P(Toss_{11} = H|Previous10 = H) = P(Toss_{11} = H)$.\n",
    "      * $P(Toss_{11} = H) = P(A)P(Heads|A) + P(B)P(Heads|B) = 0.5*0.9 + 0.5*0.1 = 0.5$\n",
    "      * $P(Toss_{11} = H|Previous10 = H)$ should be approximately 0.9, as it is very likely that B occurred.\n",
    "      * As such, $0.5 \\neq \\sim 0.9$ and no independence between tosses.\n",
    "  \n",
    "  \n",
    "* **Independence about Collections of Events:**\n",
    "  * Intuition: information about some of the events does not change the probability related to remaining events\n",
    "  * Mathematically, $P(A_i \\cap A_j ... \\cap A_m) = P(A_i)P(A_j)...P(A_m)$ for all distinct indices i,j,...m\n",
    "\n",
    "\n",
    "* **Pairwise Independence:**  \n",
    "  * Pairwise independence: $P(A_i \\cap A_j) = P(A_i)P(A_j)$ for any i, j\n",
    "      * Also implies this for combos of 3, 4, 5, etc. (not just 2)\n",
    "  * Example: Two independent, fair coin tosses:\n",
    "    * $H_1$: first toss is heads\n",
    "    * $H_2$: second toss is heads\n",
    "    * $C$: two tosses had same result\n",
    "      * Pairwise independent: $P(H_1 \\cap C) = 1/4 = P(H_1)*P(C)$\n",
    "      * Not independent: $P(H_1 \\cap H_2 \\cap C) = 1/4 \\neq P(H_1)P(H_2)P(C) = 1/8$\n",
    "      * Intuition: $P(C|H_1) = 1/2$, but $P(C|H_1 \\cap H_2) = 1$. As such, $C$ is not independent from $H_1$ and $H_2$ **collectively**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Lecture 2 Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1 True or False:**\n",
    "\n",
    "*A. If Ω is finite and we have a discrete uniform probability law, and if B≠∅, then the conditional probability law on B, given that B occurred, is also discrete uniform.*\n",
    "* True, because the outcomes inside B maintain the same relative proportions as in the original probability law.\n",
    "\n",
    "*B. If Ω is finite and we have a discrete uniform probability law, and if B≠∅, then the conditional probability law on Ω, given that B occurred, is also discrete uniform.*\n",
    "* False. Outcomes in Ω that are outside B have zero conditional probability, so it cannot be the case that all outcomes in Ω have the same conditional probability.\n",
    "\n",
    "\n",
    "\n",
    "**#2 Let the sample space be the unit square, Ω=[0,1]^2, and let the probability of a set be the area of the set. Let A be the set of points (x,y)∈[0,1]2 for which y≤x. Let B be the set of points for which x≤1/2. Find P(A∣B).**\n",
    "\n",
    "* $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n",
    "  * $P(A \\cap B) = 1/2 * 1/2 * 1/2  = 1/8$\n",
    "  * $P(B) = 1/2$\n",
    "  * Therefore, $P(A|B) = 1/4$  \n",
    "  \n",
    "  \n",
    "**#3 True or False:**\n",
    "\n",
    "*A.* $P(A\\cap B \\cap C^c)=P(A \\cap B)P(C^c∣A \\cap B)$\n",
    "  * True\n",
    "  \n",
    "*B.* $P(A \\cap B \\cap C^c)=P(A)P(C^c∣A)P(B∣A \\cap C^c)$\n",
    "  * True\n",
    "  \n",
    "*C.* $P(A \\cap B \\cap C^c)=P(A)P(C^c \\cap A∣A)P(B∣A \\cap C^c)$\n",
    "  * True, because $P(C^c \\cap A∣A) = P(C^c|A)$\n",
    "  \n",
    "*D.* $P(A \\cap B∣C)=P(A∣C)P(B∣A \\cap C)$\n",
    "  * True --> application of multiplication rule $P(A \\cap B) = P(A)P(B|A)$ with extra prior event C\n",
    "  \n",
    "  \n",
    "**#4 We have an infinite collection of biased coins, indexed by the positive integers. Coin i has probability 2−i of being selected. A flip of coin i results in Heads with probability 3−i. We select a coin and flip it. What is the probability that the result is Heads? The geometric sum formula may be useful here:**  \n",
    "<img src=\"l2_ex4.png\", style=\"height:px;width:=250px;\">\n",
    "  \n",
    "* $P(Heads) = \\sum\\limits_{i=1}^{\\infty}P(A_i)P(Heads|A_i) = \\sum\\limits_{i=1}^{\\infty}2^{-i}3^{-i} = \\sum\\limits_{i=1}^{\\infty}\\frac{1}{6}^{i} = \\frac{1/6}{1-1/6} = \\frac{1}{5}$  \n",
    "\n",
    "\n",
    "**#5 A test for a certain rare disease is assumed to be correct 95% of the time: if a person has the disease, the test result is positive with probability 0.95, and if the person does not have the disease, the test result is negative with probability 0.95. A person drawn at random from a certain population has probability 0.001 of having the disease.**\n",
    "\n",
    "*A. Find the probability a random person tests positive:*\n",
    "  * $P(+|Has) = 0.95$\n",
    "  * $P(-|!Has) = 0.95$\n",
    "  * $P(Has) = 0.001$\n",
    "  * Infer $P(-|Has) = 0.05$, $P(+|!Has) = 0.05$, $P(!Has) = 0.999$\n",
    "  * Total Probability Theorem: $P(+) = P(Has)P(+|Has) + P(!Has)P(+|!Has) = 0.001*0.95 + 0.999*0.05 = 0.0509$\n",
    "  \n",
    "*B. Given that the person just tested positive, what is the probability he actually has the disease?*\n",
    "  * $P(Has|+) = \\frac{P(+|Has)P(Has)}{P(+)} = \\frac{0.95*0.001}{0.0509} = 0.01866 = 1.87\\%$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Lecture 3 Exercises:  \n",
    "Unfortunately, the course made the in-lecture exercises unavailable 3 weeks after archiving the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Solved Problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**#1: Conditional probability example. We roll two fair 6-sided dice. Each one of the 36 possible outcomes is assumed to be equally likely.**  \n",
    "\n",
    "*A. Find the probability that doubles are rolled (i.e., both dice have the same number).*  \n",
    "* Use discrete uniform law for these problems: $P(A) = \\frac{A}{\\Omega}$\n",
    "* Here, $P(doubles) = \\frac{6}{36} = \\frac{1}{6}$\n",
    "\n",
    "\n",
    "*B. Given that the roll results in a sum of 4 or less, find the conditional probability that doubles are rolled.*  \n",
    "* $P(double|sum \\leq 4) = \\frac{P(double \\cap sum \\leq 4)}{P(sum \\leq 4)} = \\frac{2/36}{6/36} = \\frac{1}{3}$\n",
    "  * Could also reduce \"universe\" to \"conditional universe\" (only 6 possibilities) and count doubles there (2/6)\n",
    "\n",
    "*C. Find the probability that at least one die roll is a 6.*  \n",
    "* $P(six) = \\frac{11}{36} \\to$ 6, then n gives 6 possibilites, and n then 6 also gives six but subtract one from overlap of 6 then 6 ($n \\in [1,2,3,4,5,6]$)\n",
    "\n",
    "\n",
    "*D. Given that the two dice land on different numbers, find the conditional probability that at least one die roll is a 6.*  \n",
    "* $P(six|different) = ? \\to$ use conditioning shortcut (\"conditional universe\")\n",
    "  * Remove diagonal (1,1; 2,2; 3,3...etc.) so 6 outcomes removed from 36\n",
    "  * Now, 10 outcomes with at least one 6\n",
    "    * $P(six|different) = \\frac{10}{30} = \\frac{1}{3}$\n",
    "    \n",
    "    \n",
    "**#2: A chess tournament problem. This year's Belmont chess champion is to be selected by the following procedure. Bo and Ci, the leading challengers, first play a two-game match. If one of them wins both games, he gets to play a two-game second round with Al, the current champion. Al retains his championship unless a second round is required and the challenger beats Al in both games. If Al wins the initial game of the second round, no more games are played.**\n",
    "  \n",
    "**Furthermore, we know the following:  \n",
    "∙ The probability that Bo will beat Ci in any particular game is 0.6.  \n",
    "∙ The probability that Al will beat Bo in any particular game is 0.5.  \n",
    "∙ The probability that Al will beat Ci in any particular game is 0.7.**  \n",
    "\n",
    "Assume no tie games are possible and all games are independent.\n",
    "<img src=\"sp2_1.png\", style=\"height:px;width:=400px;\">\n",
    "\n",
    "* #1. Determine the a priori probabilities that:  \n",
    "  *A. the second round will be required.*  \n",
    "    * $P(R_2) = P(BB) + P(CC) = 0.6^2 + 0.4^2 = 0.52$ (can add because events are disjoint)\n",
    "    \n",
    "  *B. Bo will win the first round.*  \n",
    "    * $P(B_1) = P(BB) = 0.36$\n",
    "    \n",
    "  *C. Al will retain his championship this year.*\n",
    "    * $P(A) = 1 - P(BvC,BvC,BvA,BvA) - P(CvB,CvB,CvA,CvA) = 1 - 0.6^2 0.5^2 - 0.4^2 0.3^2 = 0.8956$  \n",
    "    \n",
    "\n",
    "* #2. Given that the second round is required, determine the conditional probabilities that:  \n",
    "  *A. Bo is the surviving challenger.*  \n",
    "    * $P(B|R_2) = \\frac{P(B \\cap R_2)}{P(R_2)} = \\frac{0.6^2}{0.52} = 0.6923$\n",
    "    \n",
    "  *B. Al retains his championship.*  \n",
    "    * $P(A|R_2) = \\frac{P(A \\cap R_2)}{P(R_2)} = \\frac{0.6^2*0.5 + 0.6^2*0.5^2 + 0.4^2*0.7 + 0.4^2*0.3*0.7}{0.52} = 0.7992$\n",
    "    \n",
    "\n",
    "* #3. Given that the second round was required and that it comprised only one game, what is the conditional probability that it was Bo who won the first round?  \n",
    "  * $P(B|R_2 \\cap 1G) = \\frac{P(B \\cap R_2 \\cap 1G)}{P(R_2 \\cap 1G}$\n",
    "    * Denominator: $P(R_2 \\cap 1G) = 0.4^2*0.7 + 0.6^2*0.5$\n",
    "    * Numerator: $P(B \\cap R_2 \\cap 1G) = 0.6^2*0.5$\n",
    "  * Total: $P(B|R_2 \\cap 1G) = 0.6164$\n",
    "\n",
    "\n",
    "**#3: A coin tossing puzzle. A coin is tossed twice. Alice claims that the event of getting two Heads is at least as likely if we know that the first toss is Heads than if we know that at least one of the tosses is Heads. Is she right? Does it make a difference if the coin is fair or unfair? How can we generalize Alice's reasoning?**  \n",
    "\n",
    "* What Alice assumes: $P(A \\cap B|A) \\geq P(A \\cap B| A \\cup B)$\n",
    "  * A: 1st toss is head\n",
    "  * B: 2nd toss is head  \n",
    "  \n",
    "  \n",
    "* LHS: $\\frac{P(A \\cap B \\cap A)}{P(A)} = \\frac{P(A \\cap B)}{P(A)}$  \n",
    "\n",
    "\n",
    "\n",
    "* RHS: $\\frac{P((A \\cap B) \\cap (A \\cup B))}{P(A \\cup B)} = \\frac{P(A \\cap B)}{P(A \\cup B)}$  \n",
    "\n",
    "  * Since $A \\subset (A \\cup B) \\to P(A) \\leq P(A \\cup B)$  \n",
    "  \n",
    "  \n",
    "  \n",
    "* Therefore, Alice is correct.  To generalize her reasoning:  \n",
    "\n",
    "  * Events C, D, E:\n",
    "  \n",
    "    * $D \\subset E$\n",
    "    * $C \\cap D = C \\cap E$  \n",
    "    \n",
    "  * Therefore, $P(C|D) \\geq P(C|E)$ (write out same fractional form to logic it out)\n",
    "\n",
    "\n",
    "**#4: The Monty Hall problem. This is a much discussed puzzle, based on an old American game show. You are told that a prize is equally likely to be found behind any one of three closed doors in front of you. You point to one of the doors. A friend opens for you one of the remaining two doors, after making sure that the prize is not behind it. At this point, you can stick to your initial choice, or switch to the other unopened door. You win the prize if it lies behind your final choice of a door. Consider the following strategies:**\n",
    "* **Stick to your initial choice.**\n",
    "* ** Switch to the other unopened door.**\n",
    "* **You first point to door 1. If door 2 is opened, you do not switch. If door 3 is opened, you switch.**  \n",
    "\n",
    "**Which is the best strategy?**\n",
    "\n",
    "* From TA: \n",
    "<img src=\"sp2_4.png\", style=\"height:px;width:=300px;\">  \n",
    "\n",
    "\n",
    "* Goes over another strategy, (choose 1, stay if friend opens 2, switch if opens 3). Turns out this is at MOST equal to switching all the time but always better than staying all the time.\n",
    "\n",
    "\n",
    "**#5: A random walker. Imagine a drunk tightrope walker, who manages to keep his balance, but takes a step forward with probability p and takes a step back with probability (1−p).**\n",
    "\n",
    "* *A. What is the probability that after two steps, the tightrope walker will be at the same place on the rope as where he started?*  \n",
    "  * Two possibilities after 2 steps: $A = \\{BF, FB\\}$.\n",
    "    * Because events are disjoint, $P(A) = P(BF) + P(FB)$\n",
    "    * Because F & B are independant, $P(A) = P(B)P(F) + P(F)P(B) = 2p(1-p)$\n",
    "\n",
    "\n",
    "* *B. What is the probability that after three steps, the tightrope walker will be one step forward from where he started?*  \n",
    "  * Three possibilities after 3 steps: $C = \\{FFB, FBF, BFF\\}$\n",
    "    * Following same logic (disjoint and independant), $P(C) = 3p^2 (1-p)$\n",
    "\n",
    "\n",
    "* *C. Given that after three steps he has managed to move ahead one step, what is the probability that the first step he took was a step forward?*  \n",
    "  * Of C, 2 possibilites: $D = \\{FFB, FBF\\}$\n",
    "    * $P(D|C) = \\frac{P(D \\cap C)}{P(C)}$. Since $D \\subset C$:\n",
    "      * $P(D|C) = \\frac{P(D)}{P(C)} = \\frac{2}{3}$  \n",
    "      \n",
    "      \n",
    "**#6: **\n",
    "<img src=\"sp2_6_1.png\", style=\"height:px;width:=1000px;\">   \n",
    "  \n",
    "  \n",
    "* From TA:  \n",
    "<img src=\"sp2_6_2.png\", style=\"height:px;width:=400px;\">\n",
    "\n",
    "*A.*\n",
    "* Multiplication rule & law of total probability: $P(success) = P(0)P(success|0) + P(1)P(success|1) = p(1-\\epsilon_0) + (1-p)(1-\\epsilon_1)$  \n",
    "  \n",
    "  \n",
    "*B.*  \n",
    "* $P(1011 \\to 1011) = P(1 \\to 1 \\cap 0 \\to 0 \\cap 1 \\to 1 \\cap 1 \\to 1) = P(1 \\to 1) P(0 \\to 0) P(1 \\to 1) P(1 \\to 1)$ because of independance.\n",
    "* $P(1011 \\to 1011) = (1 - \\epsilon_0)(1 - \\epsilon_1)^3$  \n",
    "  \n",
    "  \n",
    "*C.*  \n",
    "<img src=\"sp2_6_3.png\", style=\"height:px;width:=250px;\">  \n",
    "* $P(\"0\") = (1-\\epsilon_0)^3 + 3(1-\\epsilon_0)^2 \\epsilon_0$  \n",
    "\n",
    "\n",
    "*D.*\n",
    "* $P(\"0\"|101) = \\frac{P(\"0\")P(101|\"0\")}{P(101)}$ (Bayes' Rule)  \n",
    "  * $P(101) = P(\"0\")P(101|\"0\") + P(\"1\")P(101|\"1\")$ (Law of Total Probability)\n",
    "    * $P(\"0\") = p$\n",
    "    * $P(\"1\") = 1-p$\n",
    "    * $P(101|\"0\") = (1-\\epsilon_0)\\epsilon_0^2$\n",
    "    * $P(101|\"1\") = (1-\\epsilon_1)^2 \\epsilon_1$  \n",
    "    \n",
    "    \n",
    "* Plug and chug into Bayes' Rule!\n",
    "\n",
    "\n",
    "**#7: **  \n",
    "<img src=\"sp2_7.png\", style=\"height:px;width:=1000px;\">  \n",
    "\n",
    "* In series connection: $P(a \\to b) = p^k$ (a connecting to b with k components in series)\n",
    "* In parallel connection: $P(a \\to b) = 1 - P(allFail) = 1 - (1-p)^k$ (a connecting to b with k components in parallel)\n",
    "* 1, 2, and 3 are independant from one another: $P(A \\to B) = P(1)P(2)P(3)$ where $P(i) = P(operational)$\n",
    "  * $P(1) = p$\n",
    "  * $P(3) = 1 - (1-p)^2$\n",
    "  * $P(2) = 1 - (1 - P(Top))(1 - P(Bottom))$\n",
    "    * $P(Top) = (1 - (1-p)^3)p$\n",
    "    * $P(Bottom) = p$\n",
    "    \n",
    "    \n",
    "* Plug and chug for $P(A \\to B)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Problem Set 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the course made the problem sets unavailable 3 weeks after archiving the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
